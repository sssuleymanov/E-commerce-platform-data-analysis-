# ğŸ“¦ Delivery Performance Analytics

End-to-end data analytics project demonstrating a full workflow from raw CSV files to validated Power BI dashboards using PostgreSQL and Python.

---

## ğŸ§  Project Overview

This project simulates a real-world analytics pipeline and focuses on **delivery performance analysis** using a relational dataset consisting of six connected tables.

The main goals of the project are:

* Build a clean and reliable analytical dataset
* Validate data quality and logical consistency
* Measure delivery efficiency and on-time performance
* Present clear, business-oriented insights

---

## ğŸ—‚ Data Sources

The project uses **six related CSV files**, representing a transactional e-commerce system:

* Order Ä°tems
* Customers
* Order payments
* Order
* Product summarize 
* reviews

These files were treated as raw input data and never modified directly.

---

## ğŸ” Data Pipeline

### 1ï¸âƒ£ Data Ingestion (CSV â†’ PostgreSQL)

* All raw CSV files were imported into **PostgreSQL**
* A relational schema was created to validate relationships between tables
* PostgreSQL served as a centralized and structured storage layer

### 2ï¸âƒ£ Data Cleaning & Transformation (Python)

* Data was accessed from PostgreSQL using **Python (psycopg2)**
* Exploratory analysis was performed using descriptive statistics
* Key cleaning and transformation steps:

  * Standardization of city and location names
  * Removal of irrelevant columns and records
  * Handling missing and blank values

### 3ï¸âƒ£ Curated Dataset Creation

* Cleaned and transformed tables were exported as new CSV files
* Clear separation was maintained between **raw** and **processed** datasets
* This approach improves reproducibility and transparency

### 4ï¸âƒ£ Data Modeling & Visualization (Power BI)

* Processed datasets were loaded into **Power BI**
* Additional transformations were applied where needed:

  * Converting numeric values stored as text (e.g. commas used as decimal separators)
  * Ensuring correct data types for dates and numeric fields
  * Validating table relationships
  * Detection of logically inconsistent records (e.g. delivery date earlier than approval date)

Invalid records were flagged and excluded from delivery time calculations to ensure data reliability.


---

## ğŸ“Š Key Metrics & Analysis

The dashboard focuses on a limited set of **high-impact KPIs**:

* Delivery success rate
* On-time vs late delivery distribution
* Average delivery duration for on-time / early deliveries

The number of visuals was intentionally limited to avoid visual noise and keep insights actionable.

---

## ğŸ“ˆ Key Insights

* The majority of delivered orders arrive on time or earlier
* A small portion of records contained invalid delivery timelines, indicating data quality issues in the source system
* Average delivery duration for on-time orders shows a decreasing trend over time, suggesting improvements in logistics efficiency

---

## ğŸ›  Tools & Technologies

* **PostgreSQL** â€“ relational data storage and validation
* **Python** (`psycopg2`, `pandas`) â€“ data cleaning and transformation
* **SQL** â€“ schema creation and data validation
* **Power BI** â€“ data modeling and visualization

---

## ğŸ“ Repository Structure

```text
ğŸ“¦ delivery-performance-analytics
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”œâ”€â”€ processed
â”‚   â””â”€â”€ data_dictionary.md
â”œâ”€â”€ sql
â”œâ”€â”€ python
â”œâ”€â”€ powerbi
â”œâ”€â”€ analysis
â””â”€â”€ README.m
```
